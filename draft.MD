# ğŸš€ A Comparative Study: Classical DS AHP vs. Modern Latent-Based Methods (Probabilistic PCA & VAE)

## ğŸ“Œ Overview
This repository presents a comparative study between the **Dempster-Shafer Analytic Hierarchy Process (DS AHP)** and **modern latent-based approaches** such as **Probabilistic PCA (PPCA) and Variational Autoencoders (VAE)**. The study aims to evaluate how these methods perform in terms of **feature extraction, decision-making accuracy, and computational efficiency**.

## ğŸ“– Prerequisite Knowledge
Before diving into this repository, familiarity with the following concepts is recommended:
- **Dempster-Shafer Theory (DST)**: A framework for reasoning under uncertainty, extending probability theory.
- **Analytic Hierarchy Process (AHP)**: A classical decision-making method that structures problems into hierarchies.
- **Probabilistic PCA (PPCA)**: An extension of PCA that models data probabilistically.
- **Variational Autoencoder (VAE)**: A generative model that learns latent representations by optimizing a probabilistic lower bound.
- **Machine Learning Basics**: Familiarity with probability distributions, Bayesian inference, and deep learning.

---

## ğŸ“Š Methodology
### ğŸ”¹ Classical Approach: DS AHP
- DS AHP is an extension of AHP within **Dempster-Shafer Theory**.
- It incorporates uncertainty modeling through belief functions.
- Used in **multi-criteria decision analysis (MCDA)** where alternatives are ranked based on weighted criteria.

### ğŸ”¹ Modern Latent-Based Approaches
#### **1ï¸âƒ£ Probabilistic PCA (PPCA)**
- Probabilistic formulation of PCA that introduces **Gaussian priors**.
- Captures uncertainty in **low-dimensional representations**.
- Helps in dimensionality reduction while preserving probabilistic relationships.

#### **2ï¸âƒ£ Variational Autoencoder (VAE)**
- Uses an **encoder-decoder framework** to model latent representations.
- Optimizes the **Evidence Lower Bound (ELBO)**, balancing **reconstruction loss** and **KL divergence**.
- Provides a generative approach to data compression and feature extraction.

---

## ğŸ”¨ Implementation Details
### âœ… **Implemented Methods**
- **DS AHP (Classical Approach)**: Structured decision-making framework.
- **Probabilistic PCA (PPCA)**: Implemented using **Scikit-learn**.
- **Variational Autoencoder (VAE)**: Implemented using **TensorFlow/Keras**.

### ğŸ—‚ **Dataset Used**
- Synthetic dataset sampled from **two distributions**:
  - **Half** from a **normal distribution** (mean = 47, range = 0-100).
  - **Half** from a **uniform distribution** (range = 0-100).
- Dataset shape: **(samples, 10, 12)**.

### ğŸ“ˆ **Evaluation Metrics**
- **Reconstruction Loss** (MSE/Binary Cross-Entropy for PPCA & VAE).
- **KL Divergence** (for VAEs, ensuring latent space regularization).
- **Computational Efficiency** (Training Time & Complexity).
- **Decision-Making Accuracy** (in DS AHP).

---

## ğŸ“Š Comparative Analysis
| Method  | Reconstruction Loss | Interpretability | Computation Time |
|---------|--------------------|-----------------|-----------------|
| DS AHP  | Not applicable     | High            | Moderate        |
| PPCA    | Moderate           | Medium          | Low             |
| VAE     | Low                | Low-Medium      | High            |

### ğŸ” **Observations**
- **DS AHP is interpretable** but does not provide latent representations.
- **PPCA is computationally efficient** but does not capture complex features.
- **VAE provides the most flexible representations** but at a higher computational cost.

---

## ğŸš€ How to Run the Code
### 1ï¸âƒ£ **Installation**
```bash
pip install numpy tensorflow scikit-learn matplotlib
```

### 2ï¸âƒ£ **Running Experiments**
```bash
python run_ds_ahp.py    # Run DS AHP
python run_ppca.py      # Run Probabilistic PCA
python run_vae.py       # Train and evaluate VAE
```

### 3ï¸âƒ£ **Interpreting Results**
- **Latent space visualization** (`latent_space_plot.png`).
- **Reconstruction errors** for PPCA & VAE.
- **Ranking results** from DS AHP.

---

## ğŸ¤ Contributing
Contributions are welcome! Feel free to **submit pull requests** for improvements, new features, or dataset enhancements.

---

## ğŸ“© Contact
For questions or collaborations, reach out via **GitHub Issues** or email at `your.email@example.com`.

---

## ğŸ“œ Citation
If you find this work useful, please cite it as:
```
@article{YourName2025,
  title={A Comparative Study of DS AHP and Modern Latent-Based Approaches},
  author={Your Name},
  year={2025},
  journal={GitHub Repository}
}
```

Happy coding! ğŸš€

